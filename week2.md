###1、back propagation算法原理理解？
BP算法有两个计算过程，前向计算过程和后向传播的过程。  
算法流程：  
1、 初始化每一层网络的权重变量，将样本内误差Ein=0和每层的梯度置为0。  
2、for 每个样本（xn,yn),n = 1,...,N,做：  
     计算每层的X（前向过程）。
     计算每一层的delta（反向传播过程）。  
     将这个样本的误差加入样本误差Ein。  
     for 每层，计算：  
       梯度更新值。  
       更新梯度。
BP算法的核心在与梯度的计算的过程，前向的过程当中，只需要按照初始化的权重来计算最后网络层的输出。然后再根据网络层输出的误差进行针对每一层的权重进行求导，根据
求得的导数更新权重的值。在这里，每一层的delta函数，例如第l层，也就是误差函数对l层的激活函数的导数（这里包含了多层的求导的递推过程）。
反向传播算法的一大优点是可以通过图来计算。详细参见Computing Gradient。


### sigmoid函数、tanh函数、ReLU函数的区别？各自的优缺点？对应的tf函数是？  
这三个函数都是激活函数，是为了引入非线性的因素，主要原因是因为线性模型的表达能力不够。

从PLA模型可知，在二维平面当中，只能够完全区分三个点的情况，但是二维平面取值可以有四组，即线性情况下无法可可分。  
实例： XOR模型 输入和输出的关系 [0,0],0; [0,1],1;[1,0],1;[1,1],0。根据这个问题，以最小化MSE为训练目标训练一个线性模型，出来的结果是y=0.5
所以我们需要一个非线性模型来转化这些特征 **这就是为什么引入被称为激活函数的固定非线性函数来实现这个目标**， 经过放射变换以后在进行非线性变换。
上面这个例子当中，可以先根据z = XW+C， W为2×2的矩阵，元素都为1， c=[0,-1]为2x1的列变量，然后经过max{0，z}的激活函数（非线性变化）最后得到数据为
[0,0],0; [0,1],1;[1,0],1;[2,1],0, 这个时候就可以根据异或进行运算了。  

通俗的理解：  
如果在神经网络当中没有非线性的激活函数，实际上神经网络每一层的变化都可以看成是一个放射变化，那么不管经过多少层神经网络的变化，输出都是输入的线性组合，
实际上这还是PLA实现的功能，也就是无法分开大于d+1维的数量的数据。 为了让数据可分，引入了非线性变化，然数据进行非线性的映射直至最终可以完全分开。  

激活函数的特点：  
* 非线性
sigmoid函数
