###1、back propagation算法原理理解？
BP算法有两个计算过程，前向计算过程和后向传播的过程。  
算法流程：  
1、 初始化每一层网络的权重变量，将样本内误差Ein=0和每层的梯度置为0。  
2、for 每个样本（xn,yn),n = 1,...,N,做：  
     计算每层的X（前向过程）。
     计算每一层的delta（反向传播过程）。  
     将这个样本的误差加入样本误差Ein。  
     for 每层，计算：  
       梯度更新值。  
       更新梯度。  
       
BP算法的核心在与梯度的计算的过程，前向的过程当中，只需要按照初始化的权重来计算最后网络层的输出。然后再根据网络层输出的误差进行针对每一层的权重进行求导，根据
求得的导数更新权重的值。在这里，每一层的delta函数，例如第l层，也就是误差函数对l层的激活函数的导数（这里包含了多层的求导的递推过程）。
反向传播算法的一大优点是可以通过图来计算。详细参见Computing Gradient。


### sigmoid函数、tanh函数、ReLU函数的区别？各自的优缺点？对应的tf函数是？  
这三个函数都是激活函数，是为了引入非线性的因素，主要原因是因为线性模型的表达能力不够。

从PLA模型可知，在二维平面当中，只能够完全区分三个点的情况，但是二维平面取值可以有四组，即线性情况下无法可可分。  
实例： XOR模型 输入和输出的关系 [0,0],0; [0,1],1;[1,0],1;[1,1],0。根据这个问题，以最小化MSE为训练目标训练一个线性模型，出来的结果是y=0.5
所以我们需要一个非线性模型来转化这些特征 **这就是为什么引入被称为激活函数的固定非线性函数来实现这个目标**， 经过放射变换以后在进行非线性变换。
上面这个例子当中，可以先根据z = XW+C， W为2×2的矩阵，元素都为1， c=[0,-1]为2x1的列变量，然后经过max{0，z}的激活函数（非线性变化）最后得到数据为
[0,0],0; [0,1],1;[1,0],1;[2,1],0, 这个时候就可以根据异或进行运算了。  

通俗的理解：  
如果在神经网络当中没有非线性的激活函数，实际上神经网络每一层的变化都可以看成是一个放射变化，那么不管经过多少层神经网络的变化，输出都是输入的线性组合，
实际上这还是PLA实现的功能，也就是无法分开大于d+1维的数量的数据。 为了让数据可分，引入了非线性变化，然数据进行非线性的映射直至最终可以完全分开。  

激活函数的特点：  
* 非线性： 当激活函数是非线性的时候，两层的神经网络能够包含的目标集合就已经基本上是所有的函数了。但是如果不加激活函数，那么两层实际上还是线性变换，MLP实际上也就相当于单层的射精网络。
* 可微性： 根据神经网络的BP算法，可知神经网络需要通过梯度求导来求解最优的算法。所以只要是最终的优化方法是根据梯度求解的时候，这个性质是激活函数必须的。
* 单调性： 当激活函数是单调的时候，单层的神经网络可以保证是凸函数。(这个解释有点问题，需要进一步求证，这个条件不一定需要满足）
* f(x) 約等于 x： 当激活函数满足这个细腻改制的时候，如果参数的初始化是随机设置很小的值，神经网络的训练会比较高校，如果不满足这个性质，就需要用心设置。
* 输出值的方位： 当激活函数的输出值是**有限**的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限全值的影响;当激活函数输出是**无限**的时候，模型的训练会更加的高效，不会出现梯度消失的情况，不过此时对learning_rate的设计需要更加小心（需要更小的值).

激活函数：
ReLU（Recitified linear unit）： max(0,z)
（整流线性单元  max（0，z)）：
* 优点： * 易于优化，只要整流处于激活状态，导数都保持比较大并且一致，易于训练学习。相较于tanh和sigmoid对于随机梯度下降的收敛有巨大的加速作用。 因为是由于它的非线性， 非饱和的公式。
       * sigmoid和tanh需要计算指数等，计算复杂度高，ReLU只需要一个阈值就可以得到激活值。计算量比较小。
* 缺点: 不能通过基于梯度学习的方法学习哪些使他们激活函数变为0的样本。  
基于这个原因下面有三个线性整流单元的扩展
max（0，z) + alpha\*min(0,z) 如果alpha=-1就是 **绝对值整流(absolute value rectification)** ， 对图像中的对象识别，也就是寻找在输入照明极性反转下不变的特征有意义。 alpha=0.01这类的比较小的值是 **渗漏整流线性单元(Leaky ReLU)** 。 将alpha作为一个学习参数是 **参数化整流线性单元(parametric ReLU)** 。   
整流线性耽于那的另外一个扩展
**maxout单元** 将z分为魅族具有k个值的组，输出每组中最大的元素。 优点： 可以通过任意的精确度来近似任何凸函数。 



sigmoid函数：  
* 优点： 输入任意实属值，将其变换到(0,1)区间，大的负数映射为0，大的正数映射为1。
* 缺点： * sigmoid函数存在饱和区间，所以容易出现梯度消失的问题。 当输入非常大或者非常小的时候， 根据sigmoid函数的图片可以看出，神经元的梯度是接近0的，这          样权重基本上不会更新。 所以如果初始化权重过大， 那么大多数神经元将会饱和，导致神经网络几乎不学习。 **所以不鼓励sigmoid作为前馈神经网络中的隐藏单元, 在循环网络，许多概率模型以及一些自编码器当中，因为不能够使用分段线性激活函数，所以sigmoid的使用比较常见，尽管它存在饱和的问题**
       * sigmoid的输出不是0均值的， 这回导致后层的神经元的输入不是0均值的信号，这会对后面梯度的计算产生影响。 例如：假设后面的神经元的输入都是正值， 那么对权值w的局部求导就都是正值，那么反向传播的过程当中，w都会朝着一个方向更新，也就是说要么正向更新，要么负向更新，使得收敛比较缓慢。
       * 幂计算相对而言比较耗时。
> tf函数： y = 1/(1+exp(-x))
> tf.sigmoid(x,name=None) /tf.nn.sigmoid

tanh函数（双曲正切函数 hyperbolic tangent)：
* 优点： 解决了sigmoid输出不是零均值的问题。  
* 缺点： 仍然具有饱和性问题。
> tf函数： y = 1/(1+exp(-x))
> tf.sigmoid(x,name=None) /tf.nn.sigmoid



参考文献：
https://blog.csdn.net/cyh_24/article/details/50593400
https://www.datageekers.com/t/zxca368-tf--week2/871
